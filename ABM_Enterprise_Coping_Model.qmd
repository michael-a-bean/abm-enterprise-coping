---
title: "Enterprise Coping Under Agricultural Price Shocks: An Agent-Based Model with LLM Decision Evaluation"
subtitle: "Technical Documentation and Validation Report"
author:
  - name: "Michael [Author]"
    affiliation: "Personal AI Infrastructure"
date: "2026-01-13"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    theme: cosmo
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Introduction

This document presents an agent-based microsimulation that investigates household enterprise entry dynamics in response to agricultural price shocks in Sub-Saharan Africa. The model follows a **generative microsimulation** approach: distributions are calibrated from LSMS-ISA panel data, synthetic households are generated from these calibrated parameters, and LLM-driven decision policies are evaluated against both direct prediction tasks and full simulation comparisons.

**Primary Research Question:** "Do LLM-based household coping decisions (given survey-like state inputs) replicate observed enterprise entry/exit patterns across contexts (countries), and how do they compare to standard statistical/ML baselines?"

**Scope and Purpose:** The model provides a computational framework for:
1. Calibrating distributional parameters from real LSMS data
2. Generating synthetic panels that match observed statistical properties
3. Evaluating LLM decision-making against ground-truth transitions
4. Comparing LLM performance to ML baselines (logistic regression, random forest, GBM)
5. Testing cross-country generalization (train Tanzania → test Ethiopia)

**Key Design Innovation:** The model implements a **multi-sample voting** approach for LLM decisions: K samples are generated at temperature T, validated against feasibility constraints, and aggregated via majority voting with configurable tie-breaking.

**Document Structure:** This document follows a modified ODD+D (Overview, Design concepts, Details + Decision) protocol, adapted for the dual evaluation approach (direct prediction + full simulation).

# Research Question

## Problem Statement

The core hypothesis under investigation is that **negative cash-crop price shocks are associated with enterprise entry as coping behavior**, with heterogeneous responses by asset holdings and credit access.

The model addresses two complementary questions:

1. **Direct Prediction:** Can LLMs predict household enterprise transitions (ENTER/EXIT/STAY) from state descriptions?
2. **Simulation Replication:** Can LLM-driven ABM simulations reproduce aggregate patterns (enterprise rates, transition distributions) observed in LSMS data?

## Classification Framework

Households are classified based on enterprise persistence across survey waves:

| Classification | Definition | Interpretation |
|----------------|------------|----------------|
| **Stayer** | Enterprise in >50% of observed waves | Persistent entrepreneur |
| **Coper** | Enterprise in ≤50% of observed waves (but >0) | Intermittent/shock-responsive |
| **None** | No enterprise participation | Non-entrepreneur |

## Evaluation Approach

The model uses two complementary evaluation tracks:

### Track A: Direct Prediction Evaluation

- Feed real LSMS household states to LLM
- Predict observed transitions (ENTER/EXIT/STAY)
- Compare to ML baselines (logistic regression, random forest, GBM)
- Metrics: accuracy, balanced accuracy, macro F1, per-class precision/recall
- Cross-country generalization: train Tanzania → test Ethiopia

### Track B: Full Simulation Comparison

- Generate synthetic panels from calibration artifacts
- Run ABM with LLM-driven decisions
- Compare aggregate patterns to LSMS stylized facts
- Focus on matching enterprise prevalence, entry/exit rates, heterogeneity patterns

# System Architecture

## Concept of Operations

The system architecture follows a layered design with calibration, synthesis, and evaluation components:

```
┌─────────────────────────────────────────────────────────────────┐
│                        ORCHESTRATION LAYER                       │
│                   (CLI interface, configuration)                 │
└─────────────────────┬───────────────────────────────────────────┘
                      │
    ┌─────────────────┼─────────────────┬───────────────────┐
    │                 │                 │                   │
    ▼                 ▼                 ▼                   ▼
┌───────────┐   ┌───────────┐   ┌───────────┐   ┌───────────────┐
│    ETL    │   │Calibration│   │  ABM-Core │   │  Evaluation   │
│  Pipeline │   │ Subsystem │   │   (Mesa)  │   │   Pipeline    │
└─────┬─────┘   └─────┬─────┘   └─────┬─────┘   └───────┬───────┘
      │               │               │                 │
      ▼               ▼               ▼                 ▼
┌─────────────────────────────────────────────────────────────────┐
│                      Parquet Data Lake                           │
│  ┌──────────────┐ ┌──────────────┐ ┌──────────────────────────┐ │
│  │  Canonical   │ │  Calibration │ │  Simulation + Prediction │ │
│  │    Tables    │ │   Artifacts  │ │        Outputs           │ │
│  └──────────────┘ └──────────────┘ └──────────────────────────┘ │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
              ┌───────────────────────────┐
              │   R/Quarto Validation     │
              │   (fixest, ggplot2)       │
              └───────────────────────────┘
```

## Data Flow

```
LSMS-ISA Release (v2.0)
         │
         ▼
    ┌─────────┐
    │   ETL   │ ← Country config (tanzania.yaml, ethiopia.yaml)
    └────┬────┘
         │
         ▼
┌─────────────────────────────────────────┐
│         Canonical Parquet Panels        │
│  • household_targets.parquet            │
│  • plot_crop.parquet, plot.parquet      │
└─────────────────┬───────────────────────┘
                  │
    ┌─────────────┼─────────────┐
    │             │             │
    ▼             ▼             ▼
┌─────────┐ ┌───────────┐ ┌──────────────┐
│Calibrate│ │ Build     │ │  Direct      │
│ Distrib │ │ Transition│ │  Prediction  │
└────┬────┘ │  Dataset  │ │  Evaluation  │
     │      └─────┬─────┘ └───────┬──────┘
     ▼            │               │
┌─────────────┐   │               │
│ Calibration │   │               │
│  Artifact   │   │               │
│   (.json)   │   │               │
└──────┬──────┘   │               │
       │          │               │
       ▼          ▼               ▼
  ┌─────────────────────────────────────────┐
  │           Synthetic Panel               │
  │    (from calibrated distributions)      │
  └────────────────┬────────────────────────┘
                   │
                   ▼
              ┌─────────┐
              │   ABM   │ ← MultiSampleLLMPolicy
              │ (Mesa)  │   (K samples, voting, caching)
              └────┬────┘
                   │
                   ▼
  ┌─────────────────────────────────────────┐
  │         Simulation Outputs              │
  │  • household_outcomes.parquet           │
  │  • manifest.json (provenance)           │
  │  • decision_cache.json (LLM cache)      │
  │  • decision_logs/*.jsonl                │
  └────────────────┬────────────────────────┘
                   │
                   ▼
              ┌─────────┐
              │ R/Quarto│
              │ Reports │
              └─────────┘
```

## Module Structure

```
src/
├── abm_enterprise/               # Core ABM package
│   ├── model.py                  # Mesa 3 model class
│   ├── cli.py                    # Typer CLI (abm command)
│   ├── outputs.py                # Output handling
│   │
│   ├── agents/
│   │   └── household.py          # HouseholdAgent state machine
│   │
│   ├── calibration/              # NEW: Calibration subsystem
│   │   ├── __init__.py           # Public API exports
│   │   ├── schemas.py            # CalibrationArtifact, DistributionSpec
│   │   └── fit.py                # fit_calibration(), distribution fitting
│   │
│   ├── policies/
│   │   ├── base.py               # BasePolicy, Action enum
│   │   ├── rule.py               # RulePolicy, CalibratedRulePolicy
│   │   ├── llm.py                # LLMPolicy, MultiSampleLLMPolicy
│   │   ├── providers.py          # Stub/Replay/Claude/OpenAI providers
│   │   ├── constraints.py        # Feasibility constraints
│   │   ├── voting.py             # NEW: majority_vote(), VoteResult
│   │   ├── cache.py              # NEW: DecisionCache with state hashing
│   │   ├── prompts.py            # Prompt building and parsing
│   │   └── logging.py            # Decision audit logging
│   │
│   ├── eval/                     # NEW: Evaluation pipeline
│   │   ├── direct_prediction.py  # TransitionDataset, predict_with_llm()
│   │   ├── baselines.py          # train_logistic(), train_random_forest()
│   │   └── metrics.py            # ClassificationMetrics, compare_models()
│   │
│   ├── data/
│   │   ├── schemas.py            # HouseholdState, SimulationConfig
│   │   └── synthetic.py          # SyntheticPanelGenerator
│   │
│   └── utils/
│       ├── rng.py                # Centralized RNG
│       ├── manifest.py           # Provenance tracking
│       └── logging.py            # Structured logging
│
└── etl/                          # Data pipeline
    ├── ingest.py                 # LSMS download or synthetic fallback
    ├── canonical.py              # Standardized Parquet tables
    └── derive.py                 # Target variable computation
```

# Operational Modes

## 4.1 Toy Mode

Runs without real LSMS data for development and CI testing.

```bash
abm run-toy --seed 42 --households 100 --waves 4
```

**Purpose:** Development, testing, pipeline verification.

## 4.2 Calibration Mode

Fits distributional parameters from LSMS-derived targets.

```bash
abm calibrate --country tanzania --data-dir data/processed --output-dir artifacts/calibration
```

**Outputs:**
- `calibration.json` - CalibrationArtifact with fitted distributions
- `calibration_manifest.json` - Provenance metadata

**Calibrated distributions:**
- Assets: Normal or log-normal (AIC-selected)
- Price shocks: Normal with per-wave optional variation
- Credit access: Logistic regression on assets
- Enterprise baseline: Observed prevalence and transition rates

## 4.3 Synthetic Simulation Mode

Generates synthetic panel from calibration artifacts, runs ABM with LLM decisions.

```bash
abm run-sim-synthetic calibration.json \
  --policy llm_o4mini \
  --n-households 1000 \
  --k-samples 5 \
  --temperature 0.6 \
  --seed 42
```

**Pipeline:**
1. Load `CalibrationArtifact` from JSON
2. Generate synthetic panel via `SyntheticPanelGenerator`
3. Initialize `EnterpriseCopingModel` with `MultiSampleLLMPolicy`
4. Run simulation, collect outcomes
5. Output `household_outcomes.parquet` + `decision_cache.json`

## 4.4 Direct Prediction Evaluation Mode

Tests LLM on real LSMS states predicting observed transitions.

```bash
abm eval-direct \
  --train-country tanzania \
  --test-country ethiopia \
  --data-dir data/processed \
  --k-samples 5 \
  --output-dir outputs/eval
```

**Outputs:**
- `predictions.parquet` - All predictions with confidence scores
- `metrics.json` - Classification metrics (accuracy, F1, etc.)
- `confusion_matrices/` - Per-model confusion matrices
- `model_comparison.csv` - LLM vs baseline comparison table

## 4.5 Full Simulation Mode

Requires ingested LSMS data, runs complete ABM with specified policy.

```bash
abm run-sim tanzania \
  --scenario baseline \
  --policy calibrated \
  --calibrate \
  --data-dir data/processed
```

# Agent Design

## HouseholdAgent State

| Variable | Type | Range | Description |
|----------|------|-------|-------------|
| `household_id` | str | - | Unique identifier |
| `wave` | int | 1-4 (TZ), 1-3 (ET) | Current survey wave |
| `assets` | float | ~[-2, +2] | Standardized asset index |
| `credit_access` | int | {0, 1} | Formal credit access |
| `enterprise_status` | EnterpriseStatus | HAS/NO_ENTERPRISE | Current status |
| `enterprise_entry` | int | {0, 1} | Entry this wave |
| `price_exposure` | float | ~[-0.5, +0.5] | Price shock exposure |
| `classification` | str | stayer/coper/none | Persistence type |

## HouseholdState Schema

```python
class HouseholdState(BaseModel):
    """Frozen state snapshot for policy decisions."""

    household_id: str
    wave: int
    assets: float
    credit_access: int
    enterprise_status: EnterpriseStatus
    price_exposure: float

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for hashing."""
        return {
            "household_id": self.household_id,
            "wave": self.wave,
            "assets": self.assets,
            "credit_access": self.credit_access,
            "enterprise_status": self.enterprise_status.value,
            "price_exposure": self.price_exposure,
        }
```

# Decision Policies

## Policy Interface

All policies implement the `BasePolicy` interface:

```python
class BasePolicy(ABC):
    @abstractmethod
    def decide(self, state: HouseholdState) -> Action:
        """Determine action given household state."""
        pass

class Action(Enum):
    ENTER_ENTERPRISE = "ENTER_ENTERPRISE"
    EXIT_ENTERPRISE = "EXIT_ENTERPRISE"
    NO_CHANGE = "NO_CHANGE"
```

## RulePolicy (Deterministic Baseline)

Threshold-based decisions calibratable from data:

```python
def decide(self, state: HouseholdState) -> Action:
    if state.enterprise_status == NO_ENTERPRISE:
        if state.price_exposure < self.price_threshold:
            if state.assets < self.asset_threshold:
                return Action.ENTER_ENTERPRISE
    return Action.NO_CHANGE
```

**Default Thresholds:**

| Parameter | Default | Calibrated Source |
|-----------|---------|-------------------|
| `price_threshold` | -0.1 | Median negative exposures |
| `asset_threshold` | 0.0 | Median asset index |
| `exit_asset_threshold` | -1.0 | 10th percentile assets |

## MultiSampleLLMPolicy (Primary Experimental)

The core innovation: multi-sample voting with constraint validation.

### Decision Pipeline

```
State → Prompt → K samples at T → Parse → Validate → Vote → Cache → Action
```

### Configuration

```python
class LLMPolicyConfig(BaseModel):
    model: str = "gpt-4o-mini"       # LLM model ID
    temperature: float = 0.6         # Sampling temperature
    k_samples: int = 5               # Samples per decision
    max_tokens: int = 150            # Max response tokens
    cache_enabled: bool = True       # Enable state-based caching
    tie_break: Action = NO_CHANGE    # Conservative tie-break
    tie_break_strategy: TieBreakStrategy = CONSERVATIVE
```

### Multi-Sample Voting

```python
def majority_vote(
    samples: list[Action],
    tie_break: Action = Action.NO_CHANGE,
    strategy: TieBreakStrategy = TieBreakStrategy.CONSERVATIVE,
) -> VoteResult:
    """Aggregate K action samples via majority voting."""

    # Count votes
    vote_counts = Counter(samples)

    # Find winner(s)
    max_count = max(vote_counts.values())
    winners = [a for a, c in vote_counts.items() if c == max_count]

    # Resolve ties
    if len(winners) > 1:
        if strategy == TieBreakStrategy.CONSERVATIVE:
            final = tie_break  # Default: NO_CHANGE
        elif strategy == TieBreakStrategy.RANDOM:
            final = random.choice(winners)
    else:
        final = winners[0]

    return VoteResult(
        final_action=final,
        vote_counts=vote_counts,
        confidence=max_count / len(samples),
        tie_broken=len(winners) > 1,
    )
```

### Decision Caching

Caching by state hash + config hash for reproducibility and cost efficiency:

```python
class DecisionCache:
    def __init__(self, enabled: bool = True, max_size: int = 10000):
        self._cache: dict[str, CachedDecision] = {}

    def get(self, state_hash: str, config_hash: str) -> VoteResult | None:
        key = f"{state_hash}:{config_hash}"
        return self._cache.get(key)

    def put(self, state_hash: str, config_hash: str, vote_result: VoteResult):
        key = f"{state_hash}:{config_hash}"
        self._cache[key] = CachedDecision(
            vote_result=vote_result,
            timestamp=datetime.now(),
        )
```

### Constraint Validation

All LLM proposals are validated against feasibility constraints:

| Constraint | Logic | Purpose |
|------------|-------|---------|
| `NoEntryIfAlreadyInEnterprise` | Cannot ENTER if status=1 | State consistency |
| `NoExitIfNotInEnterprise` | Cannot EXIT if status=0 | State consistency |
| `MinimumAssetsConstraint` | Entry requires assets ≥ -0.5 | Prevent unrealistic entry |
| `CreditRequiredConstraint` | Low-asset entry requires credit | Financing requirement |

**Constraint failure → fallback to NO_CHANGE**

# Calibration Subsystem

## CalibrationArtifact Schema

```python
class CalibrationArtifact(BaseModel):
    """Complete calibration artifact for synthetic generation."""

    country_source: str               # Calibration country
    created_at: datetime              # Timestamp
    git_commit: str                   # Reproducibility
    waves: list[int]                  # Available waves
    n_households: int                 # Sample size
    n_observations: int               # Total observations

    # Fitted distributions
    assets_distribution: DistributionSpec
    shock_distribution: DistributionSpec
    shock_distribution_by_wave: dict[int, DistributionSpec] | None
    credit_model: CreditModelSpec
    enterprise_baseline: EnterpriseBaseline
    transition_rates: TransitionRates
```

## Distribution Fitting

```python
def fit_calibration(
    targets_df: pd.DataFrame,
    country: str,
) -> CalibrationArtifact:
    """Fit all calibration parameters from LSMS targets."""

    # Fit asset distribution (normal or lognormal via AIC)
    assets_dist = fit_asset_distribution(targets_df["assets_index"])

    # Fit shock distribution
    shock_dist = fit_shock_distribution(targets_df["price_exposure"])

    # Fit credit model (logistic on assets)
    credit_model = fit_credit_model(
        X=targets_df[["assets_index"]],
        y=targets_df["credit_access"],
    )

    # Compute transition rates
    transition_rates = compute_transition_rates(targets_df)

    return CalibrationArtifact(
        country_source=country,
        assets_distribution=assets_dist,
        shock_distribution=shock_dist,
        credit_model=credit_model,
        transition_rates=transition_rates,
        ...
    )
```

## Synthetic Panel Generation

```python
class SyntheticPanelGenerator:
    """Generate synthetic household panels from calibration."""

    def __init__(self, calibration: CalibrationArtifact, config: SyntheticPanelConfig):
        self.calibration = calibration
        self.config = config

    def generate(self) -> pd.DataFrame:
        """Generate complete synthetic panel."""

        # Draw household-level characteristics
        assets = self._sample_assets(self.config.n_households)
        credit = self._sample_credit(assets)

        # Generate panel structure
        panel = []
        for wave in self.calibration.waves:
            shock = self._sample_shocks(wave)
            enterprise = self._initialize_enterprise(assets, credit, wave)

            panel.append({
                "wave": wave,
                "assets_index": assets,
                "credit_access": credit,
                "price_exposure": shock,
                "enterprise_status": enterprise,
            })

        return pd.concat(panel)
```

# Evaluation Pipeline

## Direct Prediction

### Transition Dataset Construction

```python
def build_transition_dataset(
    country: str,
    data_dir: Path,
) -> pd.DataFrame:
    """Build transition dataset from LSMS targets.

    Each row: (state_t, transition_label) where:
    - state_t: household state at time t
    - transition_label: ENTER/EXIT/STAY based on status change
    """

    transitions = []
    for hh_id, hh_df in targets.groupby("household_id"):
        for i in range(len(hh_df) - 1):
            row_t = hh_df.iloc[i]
            row_t1 = hh_df.iloc[i + 1]

            transition = compute_transition_label(
                row_t["enterprise_status"],
                row_t1["enterprise_status"],
            )

            transitions.append({
                "household_id": hh_id,
                "wave_t": row_t["wave"],
                "assets_index": row_t["assets_index"],
                "credit_access": row_t["credit_access"],
                "enterprise_status": row_t["enterprise_status"],
                "price_exposure": row_t["price_exposure"],
                "transition": transition,
            })

    return pd.DataFrame(transitions)
```

### LLM Prediction

```python
def predict_with_llm(
    dataset: pd.DataFrame,
    policy: MultiSampleLLMPolicy,
) -> pd.DataFrame:
    """Run LLM predictions on transition dataset."""

    predictions = []
    for idx, row in dataset.iterrows():
        state = state_from_row(row)
        action = policy.decide(state)
        predicted_transition = action_to_transition(action, row["enterprise_status"])

        predictions.append({
            "llm_action": action.value,
            "llm_transition": predicted_transition.value,
            "llm_confidence": policy.cache.get_confidence(state),
        })

    return dataset.assign(**pd.DataFrame(predictions))
```

## Baseline Models

```python
def train_baselines(
    train_df: pd.DataFrame,
    feature_cols: list[str],
) -> dict[str, BaseEstimator]:
    """Train baseline classifiers for comparison."""

    X = train_df[feature_cols]
    y = train_df["transition"]

    return {
        "logistic": LogisticRegression().fit(X, y),
        "random_forest": RandomForestClassifier(n_estimators=100).fit(X, y),
        "gradient_boosting": GradientBoostingClassifier().fit(X, y),
    }
```

## Classification Metrics

```python
@dataclass
class ClassificationMetrics:
    accuracy: float
    balanced_accuracy: float
    macro_f1: float
    weighted_f1: float
    per_class_precision: dict[str, float]
    per_class_recall: dict[str, float]
    per_class_f1: dict[str, float]
    confusion_matrix: np.ndarray
    n_samples: int
    brier_score: float | None = None
    log_loss_value: float | None = None
```

## Model Comparison

```python
def compare_models(
    df: pd.DataFrame,
    y_true_col: str,
    model_cols: list[str],  # ["llm_transition", "logistic_transition", ...]
) -> pd.DataFrame:
    """Compare multiple models' performance."""

    comparisons = []
    for col in model_cols:
        metrics = compute_classification_metrics(
            df[y_true_col],
            df[col],
        )
        comparisons.append({
            "model": col.replace("_transition", ""),
            "accuracy": metrics.accuracy,
            "balanced_accuracy": metrics.balanced_accuracy,
            "macro_f1": metrics.macro_f1,
            "f1_ENTER": metrics.per_class_f1["ENTER"],
            "f1_EXIT": metrics.per_class_f1["EXIT"],
            "f1_STAY": metrics.per_class_f1["STAY"],
        })

    return pd.DataFrame(comparisons).sort_values("macro_f1", ascending=False)
```

# Validation Strategy

## Primary Estimand (Regression-Based)

$$\text{enterprise\_entry}_{it} = \beta_1 \times \text{price\_exposure}_{it} + \alpha_i + \gamma_t + \varepsilon_{it}$$

- Target: $\beta_1 < 0$ (adverse price shocks increase enterprise entry)
- Implemented via `fixest::feols()` with household-clustered SEs

## Direct Prediction Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| Accuracy | Overall correct predictions | >50% (better than majority class) |
| Balanced Accuracy | Average of per-class recall | >40% (above chance) |
| Macro F1 | Unweighted average F1 | >0.35 |
| Per-class F1 | F1 for ENTER/EXIT/STAY | ENTER, EXIT: >0.2 |

## Simulation Comparison Metrics

| Metric | Description | Acceptance |
|--------|-------------|------------|
| Enterprise prevalence | Rate by wave | Within 10pp of observed |
| Entry rate | Proportion entering | Within 5pp |
| Exit rate | Proportion exiting | Within 5pp |
| Coefficient sign | $\beta_1$ in FE regression | Match observed sign |

## Cross-Country Generalization

The direct prediction evaluation supports cross-country testing:

1. **Train:** Build ML baselines on Tanzania transitions
2. **Test:** Evaluate on Ethiopia transitions
3. **Compare:** LLM (no training) vs ML baselines (trained on Tanzania)

**Hypothesis:** LLMs may generalize better than ML baselines because they don't overfit to country-specific patterns.

# CLI Reference

## Primary Commands

```bash
# Calibration
abm calibrate --country tanzania --data-dir data/processed --output-dir artifacts/calibration

# Synthetic simulation with LLM
abm run-sim-synthetic artifacts/calibration/tanzania/calibration.json \
  --policy llm_o4mini \
  --n-households 1000 \
  --k-samples 5 \
  --temperature 0.6

# Direct prediction evaluation
abm eval-direct \
  --train-country tanzania \
  --test-country ethiopia \
  --data-dir data/processed \
  --k-samples 5

# Full simulation with derived targets
abm run-sim tanzania --scenario baseline --policy calibrated --calibrate

# Toy mode for testing
abm run-toy --seed 42 --households 100
```

## Makefile Targets

```bash
make setup              # Install Python dependencies
make setup-r            # Restore R environment
make test               # Run pytest (217 tests)
make lint               # Code quality checks

make ingest-data country=tanzania    # Download LSMS or synthetic
make derive-targets country=tanzania # Compute derived targets
make calibrate country=tanzania      # Fit calibration artifact

make run-toy            # Quick synthetic test
make run-sim-synthetic  # Synthetic simulation with LLM
make eval-direct        # Direct prediction evaluation

make render-report      # Generate validation report
```

# Reproducibility

## Seeding and Determinism

- All random operations use centralized RNG via `utils/rng.py`
- Seeds recorded in `manifest.json` for every run
- LLM decisions cached by state hash for reproducibility

## Manifest Structure

```json
{
  "run_id": "a1b2c3d4",
  "git_commit": "8c5720f...",
  "timestamp": "2026-01-13T22:10:30Z",
  "config": {
    "country": "tanzania",
    "scenario": "synthetic",
    "seed": 42,
    "num_waves": 4
  },
  "policy": {
    "type": "MultiSampleLLMPolicy",
    "model": "gpt-4o-mini",
    "k_samples": 5,
    "temperature": 0.6
  },
  "outputs": {
    "n_outcomes": 4000,
    "enterprise_rate": 0.234
  }
}
```

## Decision Logging

All LLM decisions logged to JSONL with:
- Input state + state hash
- Prompt text
- All K responses
- Vote result (counts, confidence, tie-broken)
- Final action applied
- Latency metrics

# Conclusions

## Implementation Summary

This ABM implements a computational framework for evaluating LLM decision-making against empirical household behavior:

- **Calibration subsystem** fits distributional parameters from LSMS data
- **Synthetic panel generation** creates household trajectories matching observed statistics
- **Multi-sample LLM policy** generates K proposals with majority voting
- **Decision caching** ensures reproducibility and cost efficiency
- **Dual evaluation tracks** compare direct prediction and simulation replication
- **Cross-country testing** assesses generalization (train Tanzania → test Ethiopia)

## Key Innovations

1. **Multi-sample voting:** Reduces variance in LLM decisions via K-sample aggregation
2. **Constraint validation:** Ensures LLM proposals satisfy feasibility requirements
3. **State-based caching:** Identical states produce identical decisions across runs
4. **Dual evaluation:** Direct prediction + simulation replication provide complementary evidence

## Limitations

1. **No general equilibrium:** Prices remain exogenous; no market feedback
2. **No agent interaction:** Households make independent decisions
3. **Binary outcomes:** Enterprise status is 0/1, not enterprise type
4. **LLM variability:** Results depend on model version and API behavior

## External Validity

The model is calibrated and validated against:
- **Tanzania:** LSMS-ISA National Panel Survey, 4 waves (2008-2014)
- **Ethiopia:** LSMS-ISA Socioeconomic Survey, 3 waves (2011-2015)

Generalization to other contexts requires additional validation.

# Appendix A: Code Excerpts

## A.1 MultiSampleLLMPolicy.decide()

```python
def decide(self, state: HouseholdState) -> Action:
    """Make a decision using multi-sample voting."""

    # Check cache
    state_hash = compute_state_hash(state.to_dict())
    cached = self.cache.get(state_hash, self._config_hash)
    if cached is not None:
        return cached.final_action

    # Generate K samples
    prompt = build_prompt(state, self.prompt_config)
    samples = []

    for i in range(self.config.k_samples):
        response, _ = self.provider.generate_with_timing(prompt)
        action = self._parse_action(parse_action_from_response(response))

        if action and self._composite_constraint.validate(state, action):
            samples.append(action)
        else:
            samples.append(self.config.fallback_action)

    # Aggregate via voting
    vote_result = majority_vote(
        samples,
        tie_break=self.config.tie_break,
        strategy=self.config.tie_break_strategy,
    )

    # Cache and return
    self.cache.put(state_hash, self._config_hash, vote_result)
    return vote_result.final_action
```

## A.2 CalibrationArtifact.save()

```python
def save(self, path: str) -> None:
    """Save calibration artifact to JSON file."""
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(self.model_dump(mode="json"), f, indent=2, default=str)
```

## A.3 compute_transition_label()

```python
def compute_transition_label(
    enterprise_t: int,
    enterprise_t1: int,
) -> TransitionLabel:
    """Compute transition label from enterprise status change."""
    if enterprise_t == 0 and enterprise_t1 == 1:
        return TransitionLabel.ENTER
    elif enterprise_t == 1 and enterprise_t1 == 0:
        return TransitionLabel.EXIT
    else:
        return TransitionLabel.STAY
```

# Appendix B: Validation Contract Reference

See `docs/VALIDATION_CONTRACT.md` for:
- Primary and secondary estimands
- Acceptance criteria
- Schema contracts
- Threshold sensitivity requirements

# Appendix C: Configuration Files

**`config/tanzania.yaml`:**
```yaml
country: tanzania
waves: [1, 2, 3, 4]
wave_years: {1: 2008, 2: 2010, 3: 2012, 4: 2014}
price_exposure_threshold: -0.1
stayer_threshold: 0.5
```

**`config/ethiopia.yaml`:**
```yaml
country: ethiopia
waves: [1, 2, 3]
wave_years: {1: 2011, 2: 2013, 3: 2015}
price_exposure_threshold: -0.1
stayer_threshold: 0.5
```

---

*Document generated: 2026-01-13*

*Repository: abm-enterprise-coping*

*Git commit: See manifest.json in output directory*
