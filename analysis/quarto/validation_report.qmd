---
title: "ABM Validation Report"
subtitle: "Direct Prediction and Simulation Validation"
author: "ABM Enterprise Coping Model"
date: today
params:
  output_dir: "../../outputs/toy"
  eval_dir: "../../outputs/eval"
  data_dir: "../../data/processed"
  country: "toy"
  eval_mode: "simulation"  # "simulation", "direct", or "both"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
    theme: cosmo
    embed-resources: true
execute:
  warning: false
  message: false
---

## Overview

This report validates the ABM Enterprise Coping Model using two complementary approaches:

1. **Direct Prediction Evaluation**: Tests LLM predictions on real LSMS household states against observed transitions
2. **Simulation Validation**: Compares aggregate patterns from ABM simulations to empirical stylized facts

**Validation Mode:** `r params$eval_mode`

**Country/Mode:** `r params$country`

```{r setup}
#| message: false
#| warning: false

library(arrow)
library(dplyr)
library(tidyr)
library(ggplot2)
library(fixest)
library(gt)
library(jsonlite)
library(scales)

# Source helper functions
source("../R/read_simulation.R")
source("../R/validation_helpers.R")

# Set ggplot theme
theme_set(theme_minimal(base_size = 12))

# Status formatting helpers
format_status <- function(pass) {
  if (is.na(pass)) return("N/A")
  if (pass) "PASS" else "FAIL"
}

status_color <- function(pass) {
  if (is.na(pass)) return("gray")
  if (pass) "darkgreen" else "red"
}
```

```{r load-data}
#| message: false

# Load simulation outputs (if running simulation validation)
sim_data <- NULL
outcomes <- NULL
manifest <- NULL

if (params$eval_mode %in% c("simulation", "both")) {
  sim_data <- tryCatch({
    read_simulation(params$output_dir)
  }, error = function(e) {
    message("Note: Could not load simulation outputs (", e$message, ")")
    NULL
  })

  if (!is.null(sim_data)) {
    outcomes <- sim_data$outcomes
    manifest <- sim_data$manifest
  }
}

# Load direct prediction results (if running direct prediction)
predictions <- NULL
metrics <- NULL
model_comparison <- NULL

if (params$eval_mode %in% c("direct", "both")) {
  predictions <- tryCatch({
    arrow::read_parquet(file.path(params$eval_dir, "predictions.parquet"))
  }, error = function(e) {
    message("Note: Could not load predictions (", e$message, ")")
    NULL
  })

  metrics <- tryCatch({
    jsonlite::fromJSON(file.path(params$eval_dir, "metrics.json"))
  }, error = function(e) {
    message("Note: Could not load metrics (", e$message, ")")
    NULL
  })

  model_comparison <- tryCatch({
    read.csv(file.path(params$eval_dir, "model_comparison.csv"))
  }, error = function(e) {
    message("Note: Could not load model comparison (", e$message, ")")
    NULL
  })
}

# Load empirical targets if available
targets <- tryCatch({
  if (params$country != "toy") {
    read_derived_targets(params$data_dir, params$country)
  } else {
    NULL
  }
}, error = function(e) {
  message("Note: No empirical targets loaded (", e$message, ")")
  NULL
})

validation_mode <- if (is.null(targets)) "toy" else "full"
```

# Part 1: Direct Prediction Evaluation {.tabset}

This section evaluates LLM performance on predicting household enterprise transitions directly from state descriptions.

## 1.1 Model Comparison

::: {.callout-note}
## Primary Metric: Macro F1
Due to severe class imbalance (>90% STAY), **Macro F1** is the primary evaluation metric.
Accuracy can be misleading when models simply predict the majority class.
Macro F1 weights all classes equally, ensuring ENTER and EXIT performance matters.
:::

```{r model-comparison}
#| label: tbl-model-comparison
#| tbl-cap: "LLM vs Baseline Model Performance (Ranked by Macro F1)"

if (!is.null(model_comparison)) {
  model_comparison |>
    dplyr::arrange(desc(macro_f1)) |>  # CRITICAL: Sort by Macro F1, not accuracy
    dplyr::mutate(
      rank = row_number(),
      across(c(accuracy, balanced_accuracy, macro_f1, weighted_f1,
               f1_ENTER, f1_EXIT, f1_STAY), ~round(., 3))
    ) |>
    dplyr::select(Rank = rank, Model = model,
                  `Macro F1 (Primary)` = macro_f1,  # Emphasize this is primary
                  `Balanced Acc.` = balanced_accuracy,
                  `Weighted F1` = weighted_f1,
                  Accuracy = accuracy,  # Move accuracy to end (less important)
                  `F1 ENTER` = f1_ENTER, `F1 EXIT` = f1_EXIT, `F1 STAY` = f1_STAY) |>
    gt::gt() |>
    gt::tab_header(
      title = "Model Performance Comparison",
      subtitle = "LLM vs ML Baselines - Ranked by Macro F1 (not accuracy)"
    ) |>
    gt::tab_style(
      style = list(gt::cell_fill(color = "#d4edda")),
      locations = gt::cells_body(rows = 1)  # Highlight best model
    ) |>
    gt::tab_style(
      style = list(gt::cell_fill(color = "#fff3cd")),
      locations = gt::cells_column_labels(columns = `Macro F1 (Primary)`)  # Highlight primary metric
    ) |>
    gt::tab_footnote(
      footnote = "Best model highlighted. Macro F1 is the primary metric due to class imbalance.",
      locations = gt::cells_column_labels(columns = `Macro F1 (Primary)`)
    )
} else {
  cat("Model comparison data not available. Run eval-direct first.\n")
}
```

## 1.2 Per-Class Metrics

```{r per-class-metrics}
#| label: fig-per-class
#| fig-cap: "Per-Class F1 Scores by Model"

if (!is.null(model_comparison)) {
  # Reshape for plotting
  per_class_data <- model_comparison |>
    dplyr::select(model, f1_ENTER, f1_EXIT, f1_STAY) |>
    tidyr::pivot_longer(
      cols = c(f1_ENTER, f1_EXIT, f1_STAY),
      names_to = "class",
      values_to = "f1_score"
    ) |>
    dplyr::mutate(
      class = stringr::str_replace(class, "f1_", ""),
      model = factor(model, levels = model_comparison$model)
    )

  ggplot(per_class_data, aes(x = class, y = f1_score, fill = model)) +
    geom_col(position = position_dodge(width = 0.8), alpha = 0.8, width = 0.7) +
    geom_text(aes(label = sprintf("%.2f", f1_score)),
              position = position_dodge(width = 0.8), vjust = -0.3, size = 2.5) +
    scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.1))) +
    scale_fill_brewer(palette = "Set2") +
    labs(
      title = "Per-Class F1 Scores",
      subtitle = "Performance on ENTER, EXIT, and STAY transitions",
      x = "Transition Class",
      y = "F1 Score",
      fill = "Model"
    ) +
    theme(legend.position = "bottom")
} else {
  cat("Per-class metrics not available.\n")
}
```

## 1.3 Confusion Matrices

```{r confusion-matrices}
#| label: fig-confusion
#| fig-cap: "Confusion Matrices by Model"

if (!is.null(predictions)) {
  # Get unique models from prediction columns
  pred_cols <- grep("_transition$", names(predictions), value = TRUE)

  if (length(pred_cols) > 0) {
    # Plot confusion matrix for LLM (primary model of interest)
    llm_col <- grep("llm", pred_cols, value = TRUE)[1]

    if (!is.na(llm_col) && !is.null(llm_col)) {
      cm_data <- predictions |>
        dplyr::count(transition, !!sym(llm_col)) |>
        dplyr::rename(True = transition, Predicted = !!sym(llm_col)) |>
        dplyr::mutate(
          True = factor(True, levels = c("ENTER", "EXIT", "STAY")),
          Predicted = factor(Predicted, levels = c("ENTER", "EXIT", "STAY"))
        )

      # Compute percentages
      cm_data <- cm_data |>
        dplyr::group_by(True) |>
        dplyr::mutate(pct = n / sum(n) * 100) |>
        dplyr::ungroup()

      ggplot(cm_data, aes(x = Predicted, y = True, fill = pct)) +
        geom_tile(color = "white", linewidth = 1) +
        geom_text(aes(label = sprintf("%d\n(%.1f%%)", n, pct)),
                  color = "white", size = 4) +
        scale_fill_gradient(low = "steelblue", high = "darkblue") +
        labs(
          title = "LLM Confusion Matrix",
          subtitle = "Rows = True Labels, Columns = Predicted Labels",
          x = "Predicted Transition",
          y = "True Transition",
          fill = "% of Row"
        ) +
        theme(
          legend.position = "right",
          axis.text = element_text(size = 12),
          panel.grid = element_blank()
        ) +
        coord_equal()
    }
  }
} else {
  cat("Predictions not available for confusion matrix.\n")
}
```

## 1.4 LLM Confidence Analysis

```{r llm-confidence}
#| label: fig-confidence
#| fig-cap: "LLM Decision Confidence Distribution"

if (!is.null(predictions) && "llm_confidence" %in% names(predictions)) {
  # Confidence distribution
  p1 <- ggplot(predictions, aes(x = llm_confidence)) +
    geom_histogram(bins = 20, fill = "steelblue", alpha = 0.8, color = "white") +
    geom_vline(xintercept = mean(predictions$llm_confidence, na.rm = TRUE),
               color = "red", linetype = "dashed", linewidth = 1) +
    labs(
      title = "Distribution of LLM Confidence",
      subtitle = "Red line = mean confidence",
      x = "Confidence (Vote Share)",
      y = "Count"
    )

  # Confidence by correctness
  predictions_with_correct <- predictions |>
    dplyr::mutate(
      correct = transition == llm_transition
    )

  p2 <- ggplot(predictions_with_correct, aes(x = correct, y = llm_confidence, fill = correct)) +
    geom_boxplot(alpha = 0.7) +
    scale_fill_manual(values = c("FALSE" = "tomato", "TRUE" = "darkgreen")) +
    labs(
      title = "Confidence by Prediction Correctness",
      x = "Correct Prediction",
      y = "Confidence",
      fill = "Correct"
    ) +
    theme(legend.position = "none")

  gridExtra::grid.arrange(p1, p2, ncol = 2)
} else {
  cat("LLM confidence data not available.\n")
}
```

## 1.5 Subgroup Analysis

### By Asset Quintile

```{r subgroup-analysis-assets}
#| label: tbl-subgroup-assets
#| tbl-cap: "LLM Performance by Asset Quintile"

if (!is.null(predictions) && "llm_transition" %in% names(predictions)) {
  # Calculate accuracy by asset quintile
  subgroup_metrics <- predictions |>
    dplyr::mutate(
      asset_quintile = ntile(assets_index, 5),
      correct = transition == llm_transition
    ) |>
    dplyr::group_by(asset_quintile) |>
    dplyr::summarize(
      n = n(),
      accuracy = mean(correct, na.rm = TRUE),
      enter_rate = mean(transition == "ENTER", na.rm = TRUE),
      exit_rate = mean(transition == "EXIT", na.rm = TRUE),
      .groups = "drop"
    ) |>
    dplyr::mutate(
      quintile_label = paste0("Q", asset_quintile, " (",
                               ifelse(asset_quintile == 1, "Lowest",
                                      ifelse(asset_quintile == 5, "Highest", "Middle")), ")")
    )

  subgroup_metrics |>
    dplyr::select(
      `Asset Quintile` = quintile_label,
      N = n,
      `LLM Accuracy` = accuracy,
      `Actual ENTER Rate` = enter_rate,
      `Actual EXIT Rate` = exit_rate
    ) |>
    gt::gt() |>
    gt::fmt_percent(columns = c(`LLM Accuracy`, `Actual ENTER Rate`, `Actual EXIT Rate`), decimals = 1) |>
    gt::tab_header(
      title = "LLM Performance by Asset Quintile",
      subtitle = "Accuracy and transition rates by household wealth"
    )
} else {
  cat("Subgroup analysis requires prediction data.\n")
}
```

### By Household Head Gender (Gemini Recommendation)

Female-headed households may face different enterprise entry barriers and coping strategies.
This analysis examines whether LLM predictions generalize across household head gender.

```{r subgroup-analysis-gender}
#| label: tbl-subgroup-gender
#| tbl-cap: "LLM Performance by Household Head Gender"

if (!is.null(predictions) && "llm_transition" %in% names(predictions)) {
  # Check if gender data is available
  has_gender <- "female_headed" %in% names(predictions) ||
                "head_female" %in% names(predictions) ||
                "hh_head_sex" %in% names(predictions)

  if (has_gender) {
    # Determine gender column name
    gender_col <- if ("female_headed" %in% names(predictions)) "female_headed"
                  else if ("head_female" %in% names(predictions)) "head_female"
                  else "hh_head_sex"

    gender_metrics <- predictions |>
      dplyr::mutate(
        female_headed = !!sym(gender_col),
        female_headed = if_else(female_headed == 1 | female_headed == TRUE, TRUE, FALSE),
        correct = transition == llm_transition,
        # Calculate per-class correctness for F1 approximation
        enter_correct = (transition == "ENTER") & (llm_transition == "ENTER"),
        exit_correct = (transition == "EXIT") & (llm_transition == "EXIT"),
        stay_correct = (transition == "STAY") & (llm_transition == "STAY")
      ) |>
      dplyr::group_by(female_headed) |>
      dplyr::summarize(
        n = n(),
        accuracy = mean(correct, na.rm = TRUE),
        # Per-class precision (approximation)
        enter_rate_actual = mean(transition == "ENTER", na.rm = TRUE),
        exit_rate_actual = mean(transition == "EXIT", na.rm = TRUE),
        enter_rate_pred = mean(llm_transition == "ENTER", na.rm = TRUE),
        exit_rate_pred = mean(llm_transition == "EXIT", na.rm = TRUE),
        .groups = "drop"
      ) |>
      dplyr::mutate(
        gender_label = ifelse(female_headed, "Female-Headed", "Male-Headed")
      )

    gender_metrics |>
      dplyr::select(
        `Household Type` = gender_label,
        N = n,
        `LLM Accuracy` = accuracy,
        `Actual ENTER Rate` = enter_rate_actual,
        `Predicted ENTER Rate` = enter_rate_pred,
        `Actual EXIT Rate` = exit_rate_actual,
        `Predicted EXIT Rate` = exit_rate_pred
      ) |>
      gt::gt() |>
      gt::fmt_percent(columns = c(`LLM Accuracy`, `Actual ENTER Rate`, `Predicted ENTER Rate`,
                                  `Actual EXIT Rate`, `Predicted EXIT Rate`), decimals = 1) |>
      gt::tab_header(
        title = "LLM Performance by Household Head Gender",
        subtitle = "Gender-disaggregated accuracy and transition rates"
      ) |>
      gt::tab_footnote(
        footnote = "Female-headed households may face different enterprise barriers (capital, market access, time constraints).",
        locations = gt::cells_column_labels(columns = `Household Type`)
      )

    # Gender parity check
    if (nrow(gender_metrics) == 2) {
      female_acc <- gender_metrics |> dplyr::filter(female_headed) |> dplyr::pull(accuracy)
      male_acc <- gender_metrics |> dplyr::filter(!female_headed) |> dplyr::pull(accuracy)
      parity_gap <- abs(female_acc - male_acc)

      cat(sprintf("\n--- Gender Parity Analysis ---\n"))
      cat(sprintf("Female-headed HH accuracy: %.1f%%\n", female_acc * 100))
      cat(sprintf("Male-headed HH accuracy: %.1f%%\n", male_acc * 100))
      cat(sprintf("Parity gap: %.1f percentage points\n", parity_gap * 100))
      cat(sprintf("Status: %s\n", ifelse(parity_gap < 0.10, "ACCEPTABLE (<10pp)", "REVIEW NEEDED (>=10pp)")))
    }
  } else {
    cat("Gender data not available in predictions.\n")
    cat("To enable this analysis, ensure household head gender is included in LSMS-derived targets.\n")
    cat("Expected column names: 'female_headed', 'head_female', or 'hh_head_sex'\n")
  }
} else {
  cat("Subgroup analysis requires prediction data.\n")
}
```

### By Credit Access

```{r subgroup-analysis-credit}
#| label: tbl-subgroup-credit
#| tbl-cap: "LLM Performance by Credit Access"

if (!is.null(predictions) && "llm_transition" %in% names(predictions) && "credit_access" %in% names(predictions)) {
  credit_metrics <- predictions |>
    dplyr::mutate(
      correct = transition == llm_transition
    ) |>
    dplyr::group_by(credit_access) |>
    dplyr::summarize(
      n = n(),
      accuracy = mean(correct, na.rm = TRUE),
      enter_rate = mean(transition == "ENTER", na.rm = TRUE),
      exit_rate = mean(transition == "EXIT", na.rm = TRUE),
      .groups = "drop"
    ) |>
    dplyr::mutate(
      credit_label = ifelse(credit_access == 1, "Has Credit Access", "No Credit Access")
    )

  credit_metrics |>
    dplyr::select(
      `Credit Status` = credit_label,
      N = n,
      `LLM Accuracy` = accuracy,
      `Actual ENTER Rate` = enter_rate,
      `Actual EXIT Rate` = exit_rate
    ) |>
    gt::gt() |>
    gt::fmt_percent(columns = c(`LLM Accuracy`, `Actual ENTER Rate`, `Actual EXIT Rate`), decimals = 1) |>
    gt::tab_header(
      title = "LLM Performance by Credit Access",
      subtitle = "Credit-disaggregated accuracy and transition rates"
    )
} else {
  cat("Credit access subgroup analysis requires prediction data with credit_access column.\n")
}
```

# Part 2: Simulation Validation {.tabset}

This section validates aggregate simulation patterns against empirical observations.

## 2.1 Manifest & Metadata

```{r manifest}
#| label: tbl-manifest
#| tbl-cap: "Simulation Parameters"

if (!is.null(manifest)) {
  manifest_df <- data.frame(
    Parameter = names(manifest),
    Value = sapply(manifest, function(x) {
      if (is.list(x)) {
        jsonlite::toJSON(x, auto_unbox = TRUE)
      } else {
        as.character(x)
      }
    }),
    stringsAsFactors = FALSE
  )

  manifest_df |>
    gt::gt() |>
    gt::tab_header(
      title = "Simulation Parameters",
      subtitle = sprintf("Run ID: %s", manifest$run_id %||% "N/A")
    ) |>
    gt::cols_width(
      Parameter ~ px(200),
      Value ~ px(400)
    )
} else {
  cat("Simulation manifest not available.\n")
}
```

## 2.2 Primary Estimand

The primary estimand tests whether negative price shocks increase enterprise entry:

$$\text{enterprise}_{it} = \beta_1 \times \text{price\_exposure}_{it} + \alpha_i + \gamma_t + \epsilon_{it}$$

**Target:** $\beta_1 < 0$ (price busts increase enterprise entry)

```{r regression-primary}
#| label: tbl-regression-primary
#| tbl-cap: "Primary Fixed Effects Regression"

if (!is.null(outcomes)) {
  fe_results <- run_fe_regression(outcomes, return_results = TRUE)
  fe_model <- fe_results$model

  summary(fe_model)

  cat("\n--- Primary Estimand Results ---\n")
  cat(sprintf("Coefficient (price_exposure): %.4f\n", fe_results$coefficient))
  cat(sprintf("Standard Error: %.4f\n", fe_results$std_error))
  cat(sprintf("t-statistic: %.4f\n", fe_results$t_statistic))
  cat(sprintf("p-value: %.4f\n", fe_results$p_value))
  cat(sprintf("\nVALIDATION STATUS: %s\n", format_status(fe_results$pass)))
} else {
  fe_results <- list(pass = NA, coefficient = NA, p_value = NA)
  cat("Simulation outcomes not available for regression.\n")
}
```

## 2.3 Heterogeneity Analysis

### Asset Interaction

Testing whether low-asset households respond more strongly to price shocks:

```{r regression-assets}
#| label: tbl-regression-assets
#| tbl-cap: "Asset Interaction Regression"

if (!is.null(outcomes)) {
  asset_results <- run_asset_interaction_regression(outcomes, return_results = TRUE)
  asset_model <- asset_results$model
  summary(asset_model)

  cat("\n--- Asset Interaction Results ---\n")
  cat(sprintf("Interaction Coefficient: %.4f\n", asset_results$coefficient))
  cat(sprintf("p-value: %.4f\n", asset_results$p_value))
  cat(sprintf("\nVALIDATION STATUS: %s\n", format_status(asset_results$pass)))
} else {
  asset_results <- list(pass = NA, coefficient = NA, p_value = NA)
  cat("Simulation outcomes not available.\n")
}
```

### Credit Access Interaction

```{r regression-credit}
#| label: tbl-regression-credit
#| tbl-cap: "Credit Access Interaction Regression"

if (!is.null(outcomes)) {
  credit_results <- run_credit_interaction_regression(outcomes, return_results = TRUE)
  credit_model <- credit_results$model
  summary(credit_model)

  cat("\n--- Credit Interaction Results ---\n")
  cat(sprintf("Interaction Coefficient: %.4f\n", credit_results$coefficient))
  cat(sprintf("p-value: %.4f\n", credit_results$p_value))
  cat(sprintf("\nVALIDATION STATUS: %s\n", format_status(credit_results$pass)))
} else {
  credit_results <- list(pass = NA, coefficient = NA, p_value = NA)
  cat("Simulation outcomes not available.\n")
}
```

## 2.4 Distribution Comparisons

### Enterprise Rate by Wave

```{r plot-enterprise-wave}
#| label: fig-enterprise-wave
#| fig-cap: "Enterprise Rate by Wave"

if (!is.null(outcomes)) {
  sim_by_wave <- outcomes |>
    group_by(wave) |>
    summarize(
      enterprise_rate = mean(as.numeric(enterprise_status), na.rm = TRUE),
      source = "Simulated",
      .groups = "drop"
    )

  if (!is.null(targets)) {
    obs_by_wave <- targets |>
      group_by(wave) |>
      summarize(
        enterprise_rate = mean(as.numeric(enterprise_indicator), na.rm = TRUE),
        source = "Observed",
        .groups = "drop"
      )
    wave_data <- bind_rows(sim_by_wave, obs_by_wave)
  } else {
    wave_data <- sim_by_wave
  }

  ggplot(wave_data, aes(x = factor(wave), y = enterprise_rate, fill = source)) +
    geom_col(position = position_dodge(width = 0.8), alpha = 0.8, width = 0.7) +
    geom_text(aes(label = sprintf("%.1f%%", enterprise_rate * 100)),
              position = position_dodge(width = 0.8), vjust = -0.5, size = 3) +
    scale_y_continuous(labels = scales::percent_format(), expand = expansion(mult = c(0, 0.15))) +
    scale_fill_manual(values = c("Simulated" = "steelblue", "Observed" = "darkgreen")) +
    labs(
      title = "Enterprise Rate by Wave",
      subtitle = if (!is.null(targets)) "Simulated vs Observed" else "Simulated Only",
      x = "Wave",
      y = "Enterprise Rate",
      fill = "Source"
    ) +
    theme(legend.position = "bottom")
} else {
  cat("Simulation outcomes not available for plotting.\n")
}
```

### Enterprise Rate by Asset Quintile

```{r plot-enterprise-assets}
#| label: fig-enterprise-assets
#| fig-cap: "Enterprise Rate by Asset Quintile"

if (!is.null(outcomes)) {
  sim_by_quintile <- outcomes |>
    mutate(asset_quintile = ntile(assets, 5)) |>
    group_by(asset_quintile) |>
    summarize(
      enterprise_rate = mean(as.numeric(enterprise_status), na.rm = TRUE),
      source = "Simulated",
      .groups = "drop"
    )

  if (!is.null(targets) && "asset_quintile" %in% names(targets)) {
    obs_by_quintile <- targets |>
      group_by(asset_quintile) |>
      summarize(
        enterprise_rate = mean(as.numeric(enterprise_indicator), na.rm = TRUE),
        source = "Observed",
        .groups = "drop"
      )
    quintile_data <- bind_rows(sim_by_quintile, obs_by_quintile)
  } else {
    quintile_data <- sim_by_quintile
  }

  ggplot(quintile_data, aes(x = factor(asset_quintile), y = enterprise_rate, fill = source)) +
    geom_col(position = position_dodge(width = 0.8), alpha = 0.8, width = 0.7) +
    scale_y_continuous(labels = scales::percent_format(), expand = expansion(mult = c(0, 0.15))) +
    scale_fill_manual(values = c("Simulated" = "steelblue", "Observed" = "darkgreen")) +
    labs(
      title = "Enterprise Rate by Asset Quintile",
      subtitle = "Q1 = Lowest Assets, Q5 = Highest Assets",
      x = "Asset Quintile",
      y = "Enterprise Rate",
      fill = "Source"
    ) +
    theme(legend.position = "bottom")
} else {
  cat("Simulation outcomes not available.\n")
}
```

## 2.5 Threshold Sensitivity

Testing robustness of conclusions across alternative stayer/coper thresholds.

```{r sensitivity-analysis}
#| label: tbl-sensitivity
#| tbl-cap: "Primary Estimand Across Classification Thresholds"

if (!is.null(outcomes)) {
  sensitivity_results <- run_threshold_sensitivity(outcomes, thresholds = c(0.33, 0.50, 0.67))

  sensitivity_results |>
    dplyr::mutate(
      threshold_label = sprintf("%.0f%%", threshold * 100),
      coefficient = sprintf("%.4f", beta_price_exposure),
      std_error = sprintf("%.4f", se_price_exposure),
      p_value_fmt = sprintf("%.4f", p_value),
      sign_status = ifelse(sign_negative, "Negative", "Positive"),
      sig_status = ifelse(significant_05, "p < 0.05", "p >= 0.05")
    ) |>
    dplyr::select(
      Threshold = threshold_label,
      Stayers = n_stayers,
      Copers = n_copers,
      `Beta (price_exposure)` = coefficient,
      `P-value` = p_value_fmt,
      Sign = sign_status
    ) |>
    gt::gt() |>
    gt::tab_header(
      title = "Threshold Sensitivity Analysis",
      subtitle = "Primary estimand across classification thresholds"
    )

  # Assess stability
  stability <- assess_threshold_stability(sensitivity_results)

  cat("\n--- Threshold Sensitivity Assessment ---\n")
  cat(sprintf("Sign stability: %s\n", if (stability$sign_stable) "STABLE" else "UNSTABLE"))
  cat(sprintf("Coefficient range: [%.4f, %.4f]\n", stability$coefficient_range[1], stability$coefficient_range[2]))
} else {
  stability <- list(stable = NA, sign_stable = NA)
  cat("Simulation outcomes not available for sensitivity analysis.\n")
}
```

# Part 3: Summary

## 3.1 Combined Validation Summary

```{r summary-table}
#| label: tbl-summary
#| tbl-cap: "Validation Summary"

summary_rows <- list()

# Direct prediction metrics (if available)
# NOTE: Macro F1 is the PRIMARY metric (Gemini recommendation for class imbalance)
if (!is.null(model_comparison)) {
  llm_row <- model_comparison |> dplyr::filter(grepl("llm", model, ignore.case = TRUE)) |> dplyr::slice(1)
  best_baseline <- model_comparison |> dplyr::filter(!grepl("llm", model, ignore.case = TRUE)) |>
    dplyr::arrange(desc(macro_f1)) |> dplyr::slice(1)  # Sort by F1, not accuracy

  if (nrow(llm_row) > 0) {
    # PRIMARY METRIC: Macro F1 (emphasized per Gemini recommendation)
    summary_rows[[length(summary_rows) + 1]] <- data.frame(
      Test = "LLM Macro F1 (PRIMARY METRIC)",
      Value = sprintf("%.3f", llm_row$macro_f1),
      Criterion = "> 0.35",
      Status = format_status(llm_row$macro_f1 > 0.35)
    )

    summary_rows[[length(summary_rows) + 1]] <- data.frame(
      Test = "LLM vs Best ML Baseline (Macro F1)",
      Value = sprintf("LLM: %.3f, Best ML: %.3f", llm_row$macro_f1, best_baseline$macro_f1),
      Criterion = "LLM >= 90% of best ML",
      Status = format_status(llm_row$macro_f1 >= best_baseline$macro_f1 * 0.9)
    )

    # Per-class F1 scores (important for imbalanced data)
    summary_rows[[length(summary_rows) + 1]] <- data.frame(
      Test = "LLM ENTER F1",
      Value = sprintf("%.3f", llm_row$f1_ENTER),
      Criterion = "> 0.20 (minority class)",
      Status = format_status(llm_row$f1_ENTER > 0.20)
    )

    summary_rows[[length(summary_rows) + 1]] <- data.frame(
      Test = "LLM EXIT F1",
      Value = sprintf("%.3f", llm_row$f1_EXIT),
      Criterion = "> 0.20 (minority class)",
      Status = format_status(llm_row$f1_EXIT > 0.20)
    )
  }
}

# Simulation validation metrics (if available)
if (!is.null(outcomes)) {
  summary_rows[[length(summary_rows) + 1]] <- data.frame(
    Test = "Primary Estimand (beta_1 < 0)",
    Value = sprintf("beta = %.4f, p = %.4f", fe_results$coefficient, fe_results$p_value),
    Criterion = "beta < 0, p < 0.05",
    Status = format_status(fe_results$pass)
  )

  summary_rows[[length(summary_rows) + 1]] <- data.frame(
    Test = "Asset Interaction (beta_2 < 0)",
    Value = sprintf("beta = %.4f, p = %.4f", asset_results$coefficient, asset_results$p_value),
    Criterion = "beta < 0, p < 0.05",
    Status = format_status(asset_results$pass)
  )

  summary_rows[[length(summary_rows) + 1]] <- data.frame(
    Test = "Credit Interaction",
    Value = sprintf("beta = %.4f, p = %.4f", credit_results$coefficient, credit_results$p_value),
    Criterion = "beta < 0",
    Status = format_status(credit_results$pass)
  )

  summary_rows[[length(summary_rows) + 1]] <- data.frame(
    Test = "Threshold Sensitivity",
    Value = if (stability$sign_stable) "Sign stable across thresholds" else "Sign varies",
    Criterion = "Consistent sign",
    Status = format_status(stability$sign_stable)
  )
}

if (length(summary_rows) > 0) {
  summary_df <- do.call(rbind, summary_rows)

  summary_df |>
    gt::gt() |>
    gt::tab_header(
      title = "Validation Summary",
      subtitle = sprintf("Mode: %s | Country: %s", params$eval_mode, params$country)
    ) |>
    gt::tab_style(
      style = list(gt::cell_fill(color = "#d4edda")),
      locations = gt::cells_body(columns = Status, rows = Status == "PASS")
    ) |>
    gt::tab_style(
      style = list(gt::cell_fill(color = "#f8d7da")),
      locations = gt::cells_body(columns = Status, rows = Status == "FAIL")
    ) |>
    gt::tab_style(
      style = list(gt::cell_fill(color = "#e2e3e5")),
      locations = gt::cells_body(columns = Status, rows = Status == "N/A")
    )
} else {
  cat("No validation results available.\n")
}
```

## 3.2 Conclusions

```{r conclusions}
#| results: hold

cat("=== VALIDATION SUMMARY ===\n\n")

# Direct prediction summary
if (!is.null(model_comparison)) {
  llm_row <- model_comparison |> dplyr::filter(grepl("llm", model, ignore.case = TRUE)) |> dplyr::slice(1)
  best_baseline <- model_comparison |> dplyr::filter(!grepl("llm", model, ignore.case = TRUE)) |> dplyr::slice(1)

  cat("DIRECT PREDICTION EVALUATION:\n")
  cat(sprintf("  - LLM Macro F1: %.3f\n", llm_row$macro_f1))
  cat(sprintf("  - Best ML Baseline: %s (F1: %.3f)\n", best_baseline$model, best_baseline$macro_f1))
  cat(sprintf("  - LLM vs Baseline: %s\n",
              if (llm_row$macro_f1 >= best_baseline$macro_f1) "LLM wins" else "ML wins"))
  cat(sprintf("  - LLM ENTER F1: %.3f\n", llm_row$f1_ENTER))
  cat(sprintf("  - LLM EXIT F1: %.3f\n", llm_row$f1_EXIT))
  cat(sprintf("  - LLM STAY F1: %.3f\n\n", llm_row$f1_STAY))
}

# Simulation summary
if (!is.null(outcomes)) {
  n_obs <- nrow(outcomes)
  n_households <- length(unique(outcomes$household_id))
  n_waves <- length(unique(outcomes$wave))

  cat("SIMULATION VALIDATION:\n")
  cat(sprintf("  - Observations: %d (%d households x %d waves)\n", n_obs, n_households, n_waves))
  cat(sprintf("  - Primary Estimand (beta_1 < 0): %s\n", format_status(fe_results$pass)))
  cat(sprintf("  - Asset Interaction (beta_2 < 0): %s\n", format_status(asset_results$pass)))
  cat(sprintf("  - Credit Interaction: %s\n", format_status(credit_results$pass)))
  cat(sprintf("  - Threshold Sensitivity: %s\n\n", if (stability$sign_stable) "ROBUST" else "VARIABLE"))
}

# Overall assessment
direct_pass <- if (!is.null(model_comparison)) {
  llm_row <- model_comparison |> dplyr::filter(grepl("llm", model, ignore.case = TRUE)) |> dplyr::slice(1)
  llm_row$macro_f1 > 0.35
} else {
  NA
}

sim_pass <- if (!is.null(outcomes)) {
  fe_results$pass && asset_results$pass && stability$sign_stable
} else {
  NA
}

if (!is.na(direct_pass) && !is.na(sim_pass)) {
  overall <- direct_pass && sim_pass
  cat(sprintf("=== OVERALL STATUS: %s ===\n", if (overall) "PASS" else "NEEDS REVIEW"))
} else if (!is.na(direct_pass)) {
  cat(sprintf("=== DIRECT PREDICTION: %s ===\n", if (direct_pass) "PASS" else "FAIL"))
} else if (!is.na(sim_pass)) {
  cat(sprintf("=== SIMULATION: %s ===\n", if (sim_pass) "PASS" else "FAIL"))
}
```

---

*Report generated on `r Sys.time()`*
