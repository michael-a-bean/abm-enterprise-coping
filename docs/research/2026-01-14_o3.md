# Deep Research Output: OpenAI o1
**Date:** 2026-01-14
**Query:** ABM manuscript methodology guidance
**Source:** OpenAI o1 via Crowdsource skill

## Summary

Synthesis of methodological guidance for structuring and reporting on an ABM that combines empirical calibration (from LSMS-ISA panel data) and exploratory counterfactual simulations.

---

## 1. Recommended Overall Structure: Main Text Versus Appendices

ABM manuscripts typically follow a "conceptual → methodological → results → discussion" structure, bolstered by ODD protocol (Grimm et al., 2006; 2010).

**Main Text:**
- Introduction and Conceptual Framework
- Model Description (high-level ODD)
- Calibration and Validation summary
- Experimental Design: Parameter Sweeps and Behavior Search
- Results and Discussion
- Limitations, Future Work, and Conclusion

**Appendices:**
- Detailed distribution-fitting results (K-S tests, QQ-plots)
- Full ODD documentation
- Parameter tables
- Extended sensitivity analyses
- LLM policy design details

---

## 2. Epistemic Boundaries and Avoiding Overclaims of Emergence

**Key Guidance:**
- Emergence in CAS generally requires agent-agent interaction and feedback loops (Epstein, 2008)
- When model uses exogenous shocks and minimal agent-agent interaction:
  - Emphasize heterogeneity of responses rather than endogenous social dynamics
  - Avoid language suggesting simulation "proves" emergent phenomena
  - Frame observed patterns as "model outputs under specified assumptions"
  - Be explicit about what is and is not captured

**Wording Guidance:**
- Use "aggregate behavior," "system-level responses," "heterogeneous outcomes" instead of "emergence," "self-organization," "co-evolution"

---

## 3. Pattern-Oriented Modeling (POM) Calibration with Heavy-Tailed Data

**When K-S tests fail:**
- Provide multiple metrics: QQ-plots, PP-plots, visual inspection
- Compare alternative distributions (Pareto, lognormal, GEV) using AIC/BIC
- Justify selection of "best-approximation" distribution even if p-values remain low
- In main text, highlight key distribution chosen; place full fitting table in appendix

**Key references:**
- Grimm & Railsback (2012)
- Clauset, Shalizi, & Newman (2009) on power-law distributions

---

## 4. Sensitivity Analysis and Parameter Sweep Reporting Norms

**Recommended practices:**
- Design of Experiments (DoE): factorial or fractional factorial designs
- Visualization: response surfaces (3D/contour plots)
- Regression/meta-modeling: fit statistical model to simulation outputs
- Uncertainty Quantification: report confidence intervals, discuss sampling variability
- Documentation: provide enough detail for independent replication

**Key references:**
- Saltelli et al. (2004, 2008)
- Railsback & Grimm (2019)

---

## 5. Behavior Search / Optimization: Avoiding Overfitting Claims

**Key guidance:**
- Partition simulation runs (train vs. test) if possible
- Use robust heuristics that handle multi-objective trade-offs
- Report fitness function/performance metric clearly
- Present aggregated results rather than hyper-tuned single cases
- Replicate each parameter set multiple times (10+ seeds recommended)
- Emphasize demonstration-of-method rather than definitive real-world optimality

**References:**
- Bankes (1993) on exploratory modeling
- Luke (2013) on metaheuristics
- Kelton et al. (2017) on simulation

---

## 6. Separating Design/Architecture from Empirical Results

**Model Design / Architecture (Methods Section):**
- Document conceptual and technical underpinnings of baseline and planned LLM-based policy
- Use diagrams/flowcharts for integration
- Emphasize LLM policy is in developmental/hypothetical stage

**Empirical Results (Results Section):**
- Present only baseline rule-based policy outputs
- Present parameter sweep findings
- Reiterate LLM-based outcomes are placeholders pending future runs

**Reference:**
- Gilbert (2008) on Agent-Based Models

---

## Actionable Summary

1. Adopt ODD framework + POM principles; place detailed calibration in appendices
2. Be explicit about limited agent interactions and exogenous shock structure
3. Show multiple goodness-of-fit measures for heavy-tailed distributions
4. Conduct structured sensitivity analyses with response surfaces and UQ
5. For behavior search, use multiple seeds and emphasize demonstration-of-method
6. Separate design/architecture from baseline empirical results

---

## Selected References

- Bankes, S. (1993). Exploratory modeling for policy analysis. Operations Research, 41(3), 435–449.
- Clauset, A., Shalizi, C. R., & Newman, M. E. J. (2009). Power-law distributions in empirical data. SIAM Review, 51(4), 661–703.
- Epstein, J. (2008). Why model? JASSS, 11(4), 12.
- Gilbert, N. (2008). Agent-Based Models. SAGE Publications.
- Grimm, V., et al. (2006). A standard protocol for describing individual-based and agent-based models. Ecological Modelling, 198(1–2), 115–126.
- Grimm, V., et al. (2010). The ODD protocol: A review and first update. Ecological Modelling, 221(23), 2760–2768.
- Grimm, V., & Railsback, S. F. (2012). Agent-Based and Individual-Based Modeling. Princeton University Press.
- Kelton, W. D., et al. (2017). Simulation with Arena. McGraw-Hill.
- Luke, S. (2013). Essentials of Metaheuristics, 2nd Edition. Lulu.
- Railsback, S. F., & Grimm, V. (2019). How to design and use agent-based models in social science.
- Saltelli, A., et al. (2004). Sensitivity Analysis in Practice. Wiley.
